{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Disaster Tweet Prediction Model (Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "import pickle\n",
    "from spacy.lang.char_classes import LIST_PUNCT\n",
    "from collections import defaultdict\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer, WordPunctTokenizer, RegexpTokenizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from normalization import text_cleaning, text_preprocessing\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from evaluation import score_df, avg_score_list\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "regexp = RegexpTokenizer(\"[\\w']+\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Acquisition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the downloaded disaster tweets dataset. The raw tweeter dataset obtained from Kaggle contains various disaster including non-natural disaster tweets, which are outside of our research scope such as airplane crash, car accidents, riot, arson, explosions, war & many others. Hence, the twitter dataset needs to be filtered by extracting natural disaster tweets only by referring to the keyword feature. The tweets will then selected according to list of natural disasters as defined by Federal Emergency Management Agency (FEMA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tweets_df = pd.read_csv(\"./data/tweets.csv\")\n",
    "raw_tweets_df['keyword'] = raw_tweets_df['keyword'].str.replace(r\"%20\", \"_\")\n",
    "\n",
    "natural_disaster_keywords = [\"avalanche\", \"bush_fires\", \"cyclone\", \"flood\", \"flooding\", \"floods\", \"flames\", \"forest_fire\", \"forest_fires\", \"drought\", \"dust_storm\", \n",
    "                            \"earthquake\", \"hail\", \"hailstorm\", \"heat_wave\", \"hurricane\", \"icestorm\", \"landslide\", \"lava\", \"lightning\", \"mudslide\", \"rainstorm\", \n",
    "                            \"sandstorm\", \"snowstorm\",  \"strong_wind\", \"storm\", \"thunder\", \"thunderstorm\", \"tornado\", \"twister\", \"typhoon\", \"tsunami\", \"violent_storm\", \n",
    "                            \"volcanic_activity\", \"volcano\", \"volcanic\", \"wildfire\", \"wildfires\", \"wild_fire\", \"wild_fires\", \"whirlwind\", \"windstorm\"]\n",
    "\n",
    "natural_disaster_keywords_df = raw_tweets_df[raw_tweets_df['keyword'].isin(natural_disaster_keywords)].reset_index(drop=True)\n",
    "non_natural_disaster_keywords_df = raw_tweets_df[~raw_tweets_df['keyword'].isin(natural_disaster_keywords)].reset_index(drop=True)\n",
    "extra_natural_disaster_df = non_natural_disaster_keywords_df[non_natural_disaster_keywords_df.apply(lambda x: bool(re.search('|'.join(natural_disaster_keywords), x.text)) and x.target == 1, axis=1)]\n",
    "filtered_natural_disaster_df = natural_disaster_keywords_df.append(extra_natural_disaster_df, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the raw dataset has been loaded, we proceed to clean it thoroughly by conducting these processes:\n",
    "1) Convert text to lowercase\n",
    "2) Remove whitespaces \n",
    "3) Remove punctuations\n",
    "4) Remove HTML tags\n",
    "5) Remove HTML entities\n",
    "6) Remove URL links\n",
    "7) Remove emoji\n",
    "8) Remove non-ASCII characters\n",
    "9) Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_natural_disaster_df[\"cleaned_text\"] = filtered_natural_disaster_df[\"text\"].apply(text_cleaning) # implementing text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the cleaned disaster twitter dataset needs to be processed further before being fitted into the model for training. The pre-processsing steps taken are as below:\n",
    "1) Expand contractions\n",
    "2) Correct spellings\n",
    "3) Remove stopwords\n",
    "4) Lemmatize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>normalized_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>665</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Hell</td>\n",
       "      <td>Washington has an avalanche rescue goat. His n...</td>\n",
       "      <td>Washington avalanche rescue goat his name Mazama</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>666</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>petition for KFC to bring back the avalanche</td>\n",
       "      <td>petition KFC bring back avalanche</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>667</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BSF ‘Borderman’ killed in avalanche: On Jan 13...</td>\n",
       "      <td>BSF Borderman kill avalanche on Jan evening pm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>668</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>Bangalore, India</td>\n",
       "      <td>1 BSF soldier killed in avalanche in Kashmir's...</td>\n",
       "      <td>BSF soldier kill avalanche Naugam sector perso...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>669</td>\n",
       "      <td>avalanche</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Very sad news coming in. 3 soldiers killed, 1 ...</td>\n",
       "      <td>very sad news come soldier kill miss avalanche...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1719</th>\n",
       "      <td>10337</td>\n",
       "      <td>trapped</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>Five soldiers were trapped under the #avalanch...</td>\n",
       "      <td>five soldier trap avalanche effort trace one m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1720</th>\n",
       "      <td>10378</td>\n",
       "      <td>trapped</td>\n",
       "      <td>Jammu And Kashmir</td>\n",
       "      <td>Border Security Force: Last evening at 8:30 pm...</td>\n",
       "      <td>Border Security Force last evening pm avalanch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>10381</td>\n",
       "      <td>trapped</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>Please pray for the safety of 5 soldiers who h...</td>\n",
       "      <td>please pray safety soldier trap avalanche Mach...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1722</th>\n",
       "      <td>10382</td>\n",
       "      <td>trapped</td>\n",
       "      <td>UNION REPUBLIC of Hindustan</td>\n",
       "      <td>Border Security Force: Last evening at 8:30 pm...</td>\n",
       "      <td>Border Security Force last evening pm avalanch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723</th>\n",
       "      <td>10387</td>\n",
       "      <td>trapped</td>\n",
       "      <td>India</td>\n",
       "      <td>Out of five soldiers trapped under the avalanc...</td>\n",
       "      <td>out five soldier trap avalanche one rescue Arm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1724 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id    keyword                     location  \\\n",
       "0       665  avalanche                         Hell   \n",
       "1       666  avalanche                 South Africa   \n",
       "2       667  avalanche                          NaN   \n",
       "3       668  avalanche             Bangalore, India   \n",
       "4       669  avalanche                          NaN   \n",
       "...     ...        ...                          ...   \n",
       "1719  10337    trapped                       Turkey   \n",
       "1720  10378    trapped            Jammu And Kashmir   \n",
       "1721  10381    trapped                Mumbai, India   \n",
       "1722  10382    trapped  UNION REPUBLIC of Hindustan   \n",
       "1723  10387    trapped                        India   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Washington has an avalanche rescue goat. His n...   \n",
       "1          petition for KFC to bring back the avalanche   \n",
       "2     BSF ‘Borderman’ killed in avalanche: On Jan 13...   \n",
       "3     1 BSF soldier killed in avalanche in Kashmir's...   \n",
       "4     Very sad news coming in. 3 soldiers killed, 1 ...   \n",
       "...                                                 ...   \n",
       "1719  Five soldiers were trapped under the #avalanch...   \n",
       "1720  Border Security Force: Last evening at 8:30 pm...   \n",
       "1721  Please pray for the safety of 5 soldiers who h...   \n",
       "1722  Border Security Force: Last evening at 8:30 pm...   \n",
       "1723  Out of five soldiers trapped under the avalanc...   \n",
       "\n",
       "                                        normalized_text  target  \n",
       "0      Washington avalanche rescue goat his name Mazama       0  \n",
       "1                     petition KFC bring back avalanche       0  \n",
       "2     BSF Borderman kill avalanche on Jan evening pm...       0  \n",
       "3     BSF soldier kill avalanche Naugam sector perso...       1  \n",
       "4     very sad news come soldier kill miss avalanche...       1  \n",
       "...                                                 ...     ...  \n",
       "1719  five soldier trap avalanche effort trace one m...       1  \n",
       "1720  Border Security Force last evening pm avalanch...       1  \n",
       "1721  please pray safety soldier trap avalanche Mach...       1  \n",
       "1722  Border Security Force last evening pm avalanch...       1  \n",
       "1723  out five soldier trap avalanche one rescue Arm...       1  \n",
       "\n",
       "[1724 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_natural_disaster_df[\"normalized_text\"] = filtered_natural_disaster_df[\"cleaned_text\"].apply(text_preprocessing) # implementing text normalization\n",
    "filtered_natural_disaster_df[[\"id\", \"keyword\", \"location\", \"text\", \"normalized_text\", \"target\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms are unable to read text input from the dataset. Therefore, the cleaned & normalized tweet contents' needs to be converted to a numeric vector format. There are two (2) methods, based on the Bag-of-Words model that will be used in this research to vectorize the text:\n",
    "1) Count\n",
    "2) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filtered_natural_disaster_df['normalized_text'].tolist()\n",
    "y = filtered_natural_disaster_df['target'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer\n",
    "- Counts the frequency of words in each document/ text and converts it into numerical feature vector, with each element in the vector representing the count of occureneces of the identified words within the document/ text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow = CountVectorizer()\n",
    "# X_train_bow = bow.fit_transform(X_train)\n",
    "# X_test_bow = bow.fit_transform(X_test)\n",
    "# X_bow = bow.fit_transform(X)\n",
    "# bow_model_evaluation_df = score_df(X_bow.toarray(), y)\n",
    "# bow_model_evaluation_df\n",
    "# bow_model_evaluation_df.to_csv(\"bow_model_evaluation.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF Vectorizer\n",
    "- Computes the relative frequency of a word's occurences in a specified document as compared to its frequency across all other documents. It factors in the importance of each word in a document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFIDF = TfidfVectorizer(ngram_range = (1, 2))\n",
    "X_train_tfidf = TFIDF.fit_transform(X_train)\n",
    "# tfidf_model_evaluation_df = score_df(X_train_tfidf.toarray(), y_train)\n",
    "# tfidf_model_evaluation_df\n",
    "# tfidf_model_evaluation_df.to_csv(\"tfidf_model_evaluation.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Oversampler with TFIDF\n",
    "\n",
    "# ROS = RandomUnderSampler(random_state=123)\n",
    "# X_train_tfidf = TFIDF.fit_transform(X_train)\n",
    "# X_test_tfidf = TFIDF.fit_transform(X_test)\n",
    "# X_train_ros, y_train_ros = ROS.fit_resample(X_train_tfidf, y_train)\n",
    "# ros_tfidf_model_evaluation_df = score_df(X_train_ros.toarray(), y_train_ros)\n",
    "# ros_tfidf_model_evaluation_df.to_csv(\"ros_model_evaluation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>Average_accuracy_score</th>\n",
       "      <th>Average_precision_score</th>\n",
       "      <th>Average_recall_score</th>\n",
       "      <th>Average_f1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.897981</td>\n",
       "      <td>0.992954</td>\n",
       "      <td>0.802439</td>\n",
       "      <td>0.874226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.871638</td>\n",
       "      <td>0.810117</td>\n",
       "      <td>0.971951</td>\n",
       "      <td>0.883284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.902857</td>\n",
       "      <td>0.961091</td>\n",
       "      <td>0.840236</td>\n",
       "      <td>0.889119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.867392</td>\n",
       "      <td>0.897199</td>\n",
       "      <td>0.832852</td>\n",
       "      <td>0.856655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Soft Voting</td>\n",
       "      <td>0.931553</td>\n",
       "      <td>0.944718</td>\n",
       "      <td>0.919512</td>\n",
       "      <td>0.929384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hard Voting</td>\n",
       "      <td>0.909577</td>\n",
       "      <td>0.982200</td>\n",
       "      <td>0.835366</td>\n",
       "      <td>0.893682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Classifier  Average_accuracy_score  Average_precision_score  \\\n",
       "0     Linear SVM                0.897981                 0.992954   \n",
       "1    Naive Bayes                0.871638                 0.810117   \n",
       "2  Random Forest                0.902857                 0.961091   \n",
       "3        XGBoost                0.867392                 0.897199   \n",
       "4    Soft Voting                0.931553                 0.944718   \n",
       "5    Hard Voting                0.909577                 0.982200   \n",
       "\n",
       "   Average_recall_score  Average_f1-score  \n",
       "0              0.802439          0.874226  \n",
       "1              0.971951          0.883284  \n",
       "2              0.840236          0.889119  \n",
       "3              0.832852          0.856655  \n",
       "4              0.919512          0.929384  \n",
       "5              0.835366          0.893682  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Synthetic Minority Oversampling Technique (SMOTE) oversampler with TFIDF\n",
    "\n",
    "smote = SMOTE(random_state=123)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)\n",
    "smote_tfidf_model_evaluation_df = score_df(X_train_smote.toarray(), y_train_smote)\n",
    "smote_tfidf_model_evaluation_df\n",
    "#smote_tfidf_model_evaluation_df.to_csv(\"smote_model_evaluation.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Deployment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know from our performance evaluation result done above that Ensemble Soft Voting gives the best performance, we will use that to train and fit it for deployment purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filtered_natural_disaster_df['normalized_text'].tolist()\n",
    "y = filtered_natural_disaster_df['target'].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)\n",
    "\n",
    "TFIDF = TfidfVectorizer(ngram_range = (1, 2))\n",
    "# pickle.dump(TFIDF, open('tfidf.pkl', 'wb'))\n",
    "smote = SMOTE(random_state=123)\n",
    "\n",
    "svm_linear = svm.SVC(probability=True, random_state = 123)\n",
    "nb = GaussianNB()\n",
    "rf = RandomForestClassifier(n_jobs = -1, random_state = 123)\n",
    "xgb = XGBClassifier(eval_metric = 'logloss', random_state = 123)\n",
    "soft_vote = VotingClassifier(estimators = [('svm', svm_linear), ('nb', nb), ('rf', rf), ('xgb',xgb)], voting = 'soft', n_jobs=-1) \n",
    "hard_vote = VotingClassifier(estimators = [('svm', svm_linear), ('nb', nb), ('rf', rf), ('xgb',xgb)], voting = 'hard', n_jobs=-1) \n",
    "\n",
    "X_train_tfidf = TFIDF.fit_transform(X_train)\n",
    "X_test_tfidf = TFIDF.transform(X_test)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('svm', SVC(probability=True, random_state=123)),\n",
       "                             ('nb', GaussianNB()),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(n_jobs=-1,\n",
       "                                                     random_state=123)),\n",
       "                             ('xgb',\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric='logloss', gamma=None,\n",
       "                                            gpu_id=None, importanc...\n",
       "                                            learning_rate=None,\n",
       "                                            max_delta_step=None, max_depth=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            predictor=None, random_state=123,\n",
       "                                            reg_alpha=None, reg_lambda=None,\n",
       "                                            scale_pos_weight=None,\n",
       "                                            subsample=None, tree_method=None,\n",
       "                                            validate_parameters=None,\n",
       "                                            verbosity=None))],\n",
       "                 n_jobs=-1, voting='soft')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_vote.fit(X_train_smote.toarray(), y_train_smote)\n",
    "# pickle.dump(soft_vote, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DisasterFakeTweet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "38eac0e775eac027649cc42765ba21d1eaae8285aae211db831b9d2cbe588f15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
